{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "### What is it? \n",
    "\n",
    "* Sentiment Analysis (SA) is the process of understanding an option (sentiment) about a given subject from written or spoken language.\n",
    "* It is one of the subfields of Natural Language Processing(NLP) that extracts opinion and attributes from text or speech. E.g. Machine Translation, Question Answering, Sentiment Analysis, Language modeling, etc. \n",
    "* Sentiment Analysis is a supervised learning technique.\n",
    "\n",
    "\n",
    "* SA usually counts on four tasks: \n",
    "   * opinion identification, identifying the text which contains an opinion. Such as positive, negative, neutral\n",
    "   * feature extraction, identifying the aspects being commented on, such as a product's price, color of the product, etc\n",
    "   * sentiment classification, whether the opinion popularity is positive, negative, or neutral\n",
    "   * visualization and summarization of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Terminology\n",
    "\n",
    "* Stop words -- Removal of words that are not important from the infomration point of view, such as'the','is','a' etc.\n",
    "* Tokenization -- Segmentation of text into words (a form of feature extraction)\n",
    "* Lemmatization -- Assigning the base forms of words(the lemma of 'spoke' is 'speak' and the lemmma of 'languages' is 'language')\n",
    "* Stemming -- Reducing a word to its stem or root form known as a lemma (car,cars,car's, cars' --> car(stem or root word))\n",
    "* Word Embedding -- Mappin gwords to vectors of numbers where words with similar meaning have a similar numerical representation.\n",
    "* Text Classification -- Assigning categories to a document or parts of it\n",
    "* N-grams -- Consideration of a group of words (phrases) rather than single words to extract meaning. Helps with better understanding of text; 'not happy' instead of 'happy';(e.g. bi-gram pertoken). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Source NLP libraries:\n",
    "\n",
    "* Many open source libraries are at our service when we want to implement NLP models:\n",
    "\n",
    "  * NLTK\n",
    "  * Spark NLP\n",
    "  * Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Natural Language Toolkit(nltk) is a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing and semantic reasoning\n",
    "'''\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "\n",
    "import os   # for getting environment variables\n",
    "os.getcwd()\n",
    "os.chdir('/Users/axa4/Documents/Supervised Learning/Sentiment-Analysis-for-fun')  #your desired directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with the provided toy dataset `data/toy_dataset.tsv` and load it into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "# get line numbers (optional)\n",
    "dataset = [line.rstrip() for line in open('./data/toy_dataset.tsv')]\n",
    "print(len(dataset))\n",
    "\n",
    "#load into dataframe\n",
    "dataset = pd.read_csv('./data/toy_dataset.tsv', delimiter='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus will store your cleaned dataset after preprocessing.\n",
    "corpus = []\n",
    "for i in range(0,26):\n",
    "    data = re.sub('[^a-zA-Z]', ' ', dataset['verified_reviews'][i] )\n",
    "    data = data.lower()\n",
    "    data = data.split()\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    data = [stemmer.stem(word) for word in data if not word in set(stopwords.words('english'))]\n",
    "    data = ' '.join(data)\n",
    "    corpus.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representing Text as Numerical Vectors: Bag of Words\n",
    "+ We first need to represent texts in a way that the learning algorithm can process. \n",
    "+ To represent each word in the dataset, we will convert the text into a matrix of token counts.\n",
    "\n",
    "#### TODO: Inspect the source code below to understand how we represent the text as a matrix for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "vectorizer = CountVectorizer(max_features=1400)\n",
    "\n",
    "# The function fit_transform() is used for dataset transformations in scikit-learn. \n",
    "# Notice that the vectorizer by default stores everything in a sparse array, and using X.toarray()shows us the dense version.\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "y = dataset.iloc[:,4].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the import `train_test_split()` from `sklearn.model_selection` split our dataset into `train` and `test` sets. Set the 'test_size' to 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# This means that X_test and y_test contains 20% of our data which we reserve for testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a Classifier \n",
    "\n",
    "+ We will use RandomForestClassifier() from scikit-learn library as our classifier. \n",
    "+ Please note, there are a number of classifiers that you can use for Sentiment Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import RandomForestClassifier() from sklearn library and create an instance of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "sentiment_classifier = RandomForestClassifier(n_estimators = 100, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit and Predict the test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In scikit-learn, an estimator for classification is a Python object that implements the methods fit(X, y) and predict(T)\n",
    "sentiment_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Here we are predicting the test set results\n",
    "y_pred = sentiment_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue to **exercise2.ipynb**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
